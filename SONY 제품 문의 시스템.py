import os
import torch
import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_community.tools.tavily_search import TavilySearchResults
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import HuggingFacePipeline


# âœ… GPU ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… Tavily Web ê²€ìƒ‰ ì„¤ì •
os.environ["TAVILY_API_KEY"] = "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"
web_search_tool = TavilySearchResults(k=2)

# âœ… ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(
    model_name='sentence-transformers/paraphrase-MiniLM-L3-v2',
    model_kwargs={'device': 'cpu'}  # GPUê°€ ë¶€ì¡±í•˜ë©´ cpu ì‚¬ìš©
)
directory_path = 'SONY_Chatbot/SONY vectorstore ìƒì„±'
vectorstore = Chroma(persist_directory=directory_path, embedding_function=embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={'k': 3})

# âœ… ìœ ì‚¬ë„ ëª¨ë¸ ì„¤ì • (product name í•„í„°ìš©)
similarity_model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')

# âœ… LLM ì„¤ì • (OpenChat 3.5)
model_name = "openchat/openchat_3.5"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

gen_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1,
    do_sample=True
)

llm = HuggingFacePipeline(pipeline=gen_pipeline)

# âœ… LLMChain í”„ë¡¬í”„íŠ¸
prompt = PromptTemplate(
    input_variables=["question"],
    template=(
        "You are a helpful assistant that summarizes and explains information in a simple and clear way.\n"
        "Based on the document or web search, please answer the following question clearly and concisely:\n\n"
        "{question}"
    )
)

chain = LLMChain(prompt=prompt, llm=llm)

# âœ… ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def get_answer_with_web_search(query, product_name):
    all_docs = retriever.get_relevant_documents(query)

    doc_titles = [
        doc.metadata.get("file_name", doc.page_content[:30]).split('.')[0]
        for doc in all_docs
    ] if all_docs else []

    product_embedding = similarity_model.encode(product_name)
    doc_embeddings = similarity_model.encode(doc_titles, batch_size=4) if doc_titles else []

    filtered_docs = [
        doc for doc, sim in zip(all_docs, cosine_similarity([product_embedding], doc_embeddings)[0])
        if sim >= 0.7
    ] if doc_embeddings else []

    # ë¬¸ì„œ ì‘ë‹µ
    if filtered_docs:
        context = "\n".join([doc.page_content for doc in filtered_docs])
        full_prompt = f""" ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‰½ê²Œ ìš”ì•½í•˜ê³  ì„¤ëª…í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ì„œëŠ” ì†Œë‹ˆ ì œí’ˆ '{product_name}'ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ì§ˆë¬¸: "{query}"
        ì´ ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ë§Œ ë½‘ì•„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”:
        ë¬¸ì„œ ë‚´ìš©: {context} """
        doc_answer = chain.run({"question": full_prompt}).strip()

        source_info = "\n".join([
            f"ì¶œì²˜ - íŒŒì¼ëª…: {doc.metadata.get('file_name', 'ì•Œ ìˆ˜ ì—†ìŒ')}, í˜ì´ì§€ ë²ˆí˜¸: {doc.metadata.get('page_number', 'ë¯¸í‘œê¸°')}"
            for doc in filtered_docs
        ])
    else:
        doc_answer = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        source_info = "ì¶œì²˜ ì—†ìŒ"

    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    web_results = web_search_tool.run(query)
    st.write("ğŸ” ì›¹ ê²€ìƒ‰ ê²°ê³¼ (raw):", web_results)

    if not web_results:
        web_summary = "ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    else:
        web_context = ""
        for i, result in enumerate(web_results[:3]):
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            if snippet:
                web_context += f"{i+1}. {title} - {snippet}\n"

        web_prompt = f"ë‹¤ìŒì€ '{query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n\n{web_context}"
        web_summary = chain.run({"question": web_prompt}).strip()

    combined_answer = doc_answer + "\n\nğŸ“¡ ì›¹ ê²€ìƒ‰ ìš”ì•½:\n" + web_summary
    return combined_answer, source_info

# âœ… Streamlit UI
st.title("ì†Œë‹ˆ ì œí’ˆ ì„¤ëª…ì„œ ì±—ë´‡")
product_name = st.selectbox(
    "ì œí’ˆëª…ì„ ì„ íƒí•˜ì„¸ìš”:",
    [
        "ê¸€ë˜ìŠ¤ ì‚¬ìš´ë“œ ìŠ¤í”¼ì»¤ LSPX-S3",
        "ë””ì§€í„¸ ì¹´ë©”ë¼ A7",
        "ë Œì¦ˆ êµí™˜ ê°€ëŠ¥ ë””ì§€í„¸ ì¹´ë©”ë¼ ILCE-7CM2",
        "ë¬´ì„  ë…¸ì´ì¦ˆ ì œê±° ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ WF-1000XM5",
        "ë¬´ì„  ë…¸ì´ì¦ˆ ì œê±° ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ WH-1000XM5",
        "ë¬´ì„  ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ Float Run",
        "ë¬´ì„  ìŠ¤í”¼ì»¤ SRS-XE300"
    ]
)
query = st.text_input("ì œí’ˆ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        answer, source_info = get_answer_with_web_search(query, product_name)
    st.subheader("ğŸ“© ì§ˆë¬¸")
    st.write(query)
    st.subheader("ğŸ’¬ ë‹µë³€")
    st.markdown(answer)
    st.subheader("ğŸ“ ì¶œì²˜")
    st.markdown(source_info)



# def get_answer_with_web_search(query, product_name):
#     all_docs = retriever.get_relevant_documents(query)
#     product_embedding = similarity_model.encode(product_name)
#     doc_embeddings = similarity_model.encode(
#         [doc.metadata.get("file_name", "").split('.')[0] for doc in all_docs]
#     )

#     filtered_docs = [
#         doc for doc, sim in zip(all_docs, cosine_similarity([product_embedding], doc_embeddings)[0])
#         if sim >= 0.7
#     ]

#     # ë¬¸ì„œ ì—†ìœ¼ë©´ â†’ ì›¹ ê²€ìƒ‰
#     if not filtered_docs:
#         web_results = web_search_tool.invoke(query)
#         snippets = [result.get('snippet', 'ê²°ê³¼ ì—†ìŒ') for result in web_results]
#         if snippets:
#             return (
#                 "ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í•œ ê²°ê³¼ë¥¼ ì›¹ ê²€ìƒ‰ì—ì„œ ë°˜í™˜í•©ë‹ˆë‹¤:\n" + "\n".join(snippets),
#                 "ì¶œì²˜: ì›¹ ê²€ìƒ‰"
#             )
#         else:
#             return "ë¬¸ì„œ ë° ì›¹ ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ì¶œì²˜ ì—†ìŒ"

#     # ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
#     context = "\n".join([doc.page_content for doc in filtered_docs])
#     prompt = f"ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ '{query}'ì— ë‹µí•´ì£¼ì„¸ìš”: {context}"
#     answer = llm(prompt)

#     source_info = "\n".join([
#         f"ì¶œì²˜ - íŒŒì¼ëª…: {doc.metadata.get('file_name')}, í˜ì´ì§€ ë²ˆí˜¸: {doc.metadata.get('page_number')}"
#         for doc in filtered_docs
#     ])

#     return answer, source_info

# # AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# def get_answer_with_web_search(query, product_name):
#     all_docs = retriever.get_relevant_documents(query)
#     product_embedding = similarity_model.encode(product_name)
#     doc_embeddings = similarity_model.encode(
#         [doc.metadata.get("file_name", "").split('.')[0] for doc in all_docs]
#     )
#     filtered_docs = [
#         doc for doc, sim in zip(all_docs, cosine_similarity([product_embedding], doc_embeddings)[0])
#         if sim >= 0.7
#     ]

#     if not filtered_docs:
#         web_results = web_search_tool.invoke(query)
#         snippets = [result.get('snippet', 'ê²°ê³¼ ì—†ìŒ') for result in web_results]
#         if snippets:
#             return (
#                 "ë¬¸ì„œì—ì„œ ì°¾ì§€ ëª»í•œ ê²°ê³¼ë¥¼ ì›¹ ê²€ìƒ‰ì—ì„œ ë°˜í™˜í•©ë‹ˆë‹¤:\n" + "\n".join(snippets),
#                 "ì¶œì²˜: ì›¹ ê²€ìƒ‰"
#             )
#         else:
#             return "ë¬¸ì„œ ë° ì›¹ ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ì¶œì²˜ ì—†ìŒ"

#     context = "\n".join([doc.page_content for doc in filtered_docs])
#     inputs = tokenizer(
#         f"ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ '{query}'ì— ë‹µí•´ì£¼ì„¸ìš”: {context}",
#         return_tensors="pt",
#         truncation=True,
#         max_length=512
#     ).to(device)

#     outputs = model.generate(inputs["input_ids"], max_new_tokens=200, do_sample=True)
#     answer = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

#     source_info = "\n".join([
#         f"ì¶œì²˜ - íŒŒì¼ëª…: {doc.metadata.get('file_name')}, í˜ì´ì§€ ë²ˆí˜¸: {doc.metadata.get('page_number')}"
#         for doc in filtered_docs
#     ])

#     return answer, source_info

# # Streamlit ì•±
# if "page" not in st.session_state:
#     st.session_state.page = "loading"
#     set_background(loading_image_path)
#     st.write("ë¡œë”© ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”!")
#     st.button("ì‹œì‘", on_click=lambda: st.session_state.update({"page": "input"}))
# else:
#     if st.session_state.page == "input":
#         set_background(input_image_path)
#         st.title("ì†Œë‹ˆ ì œí’ˆ ë¬¸ì˜ ì‹œìŠ¤í…œ")
#         product_name = st.selectbox(
#             "ì œí’ˆëª…ì„ ì„ íƒí•˜ì„¸ìš”:",
#             ["ê¸€ë˜ìŠ¤ ì‚¬ìš´ë“œ ìŠ¤í”¼ì»¤ LSPX-S3", "ë””ì§€í„¸ ì¹´ë©”ë¼ A7", "ë Œì¦ˆ êµí™˜ ê°€ëŠ¥ ë””ì§€í„¸ ì¹´ë©”ë¼ ILCE-7CM2", "ë¬´ì„  ë…¸ì´ì¦ˆ ì œê±° ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ WF-1000XM5", "ë¬´ì„  ë…¸ì´ì¦ˆ ì œê±° ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ WH-1000XM5", "ë¬´ì„  ìŠ¤í…Œë ˆì˜¤ í—¤ë“œì…‹ Float Run", "ë¬´ì„  ìŠ¤í”¼ì»¤ SRS-XE300" ]
#         )
#         query = st.text_input("ë¬¸ì˜ ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”:")
#         if st.button("ì§ˆë¬¸í•˜ê¸°"):
#             with st.spinner("ì‘ë‹µì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
#                 answer, source_info = get_answer_with_web_search(query, product_name)
#             st.subheader("ì§ˆë¬¸")
#             st.write(query)
#             st.subheader("ë‹µë³€")
#             st.write(answer)
#             st.subheader("ì¶œì²˜ ì •ë³´")
#             st.write(source_info)
